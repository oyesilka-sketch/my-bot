import requests
from bs4 import BeautifulSoup
import json
import re
import time
from datetime import datetime, timedelta
import random
from urllib.parse import urljoin, urlparse
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib

class NewsScrapingError(Exception):
    pass

class MultiNewsSource:
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'
        ]
        
        self.istanbul_keywords = [
            'istanbul', 'Ä°stanbul', 'ISTANBUL', 'Ä°STANBUL',
            'beyoÄŸlu', 'kadÄ±kÃ¶y', 'Ã¼skÃ¼dar', 'beÅŸiktaÅŸ', 'ÅŸiÅŸli',
            'fatih', 'bakÄ±rkÃ¶y', 'zeytinburnu', 'pendik', 'maltepe',
            'ataÅŸehir', 'taksim', 'sultanahmet', 'galata', 'beylikdÃ¼zÃ¼',
            'avcÄ±lar', 'bahÃ§elievler', 'baÄŸcÄ±lar', 'esenler', 'gaziosmanpaÅŸa',
            'eyÃ¼psultan', 'kaÄŸÄ±thane', 'sarÄ±yer', 'baÅŸakÅŸehir', 'bÃ¼yÃ¼kÃ§ekmece',
            'Ã§ekmekÃ¶y', 'kartal', 'kÃ¼Ã§Ã¼kÃ§ekmece', 'sancaktepe', 'silivri',
            'sultanbeyli', 'tuzla', 'umraniye', 'arnavutkÃ¶y', 'Ã§atalca',
            'esenyurt', 'gÃ¼ngÃ¶ren', 'sultangazi'
        ]
        
        # Haber siteleri yapÄ±landÄ±rmasÄ± - GÃœNCEL VE Ã‡ALIÅAN URL'LER
        self.news_sources = {
            'sondakika': {
                'base_url': 'https://www.sondakika.com',
                'istanbul_url': 'https://www.sondakika.com/istanbul/',
                'guncel_url': 'https://www.sondakika.com/guncel/',
                'enabled': True
            },
            'sozcu': {
                'base_url': 'https://www.sozcu.com.tr',
                'gundem_url': 'https://www.sozcu.com.tr/kategori/gundem/',
                'anasayfa_url': 'https://www.sozcu.com.tr',
                'enabled': True
            },
            'hurriyet': {
                'base_url': 'https://www.hurriyet.com.tr',
                'gundem_url': 'https://www.hurriyet.com.tr/gundem/',
                'anasayfa_url': 'https://www.hurriyet.com.tr/',
                'enabled': True
            },
            'milliyet': {
                'base_url': 'https://www.milliyet.com.tr',
                'gundem_url': 'https://www.milliyet.com.tr/gundem/',
                'anasayfa_url': 'https://www.milliyet.com.tr/',
                'enabled': True
            },
            'cnnturk': {
                'base_url': 'https://www.cnnturk.com',
                'anasayfa_url': 'https://www.cnnturk.com/',
                'enabled': True
            },
            'ntv': {
                'base_url': 'https://www.ntv.com.tr',
                'anasayfa_url': 'https://www.ntv.com.tr/',
                'enabled': False  # Timeout problemleri var
            },
            'haberturk': {
                'base_url': 'https://www.haberturk.com',
                'gundem_url': 'https://www.haberturk.com/gundem',
                'anasayfa_url': 'https://www.haberturk.com/',
                'enabled': True
            },
            'cumhuriyet': {
                'base_url': 'https://www.cumhuriyet.com.tr',
                'anasayfa_url': 'https://www.cumhuriyet.com.tr/',
                'enabled': True
            }
        }

    def get_random_headers(self):
        """Rastgele User-Agent ve headers dÃ¶ndÃ¼rÃ¼r"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none'
        }

    def safe_get(self, url, timeout=15, retries=2):
        """GÃ¼venli HTTP GET isteÄŸi - HÄ±zlÄ± ayarlar"""
        for attempt in range(retries):
            try:
                session = requests.Session()
                session.headers.update(self.get_random_headers())
                
                response = session.get(url, timeout=timeout, allow_redirects=True)
                response.raise_for_status()
                
                if response.status_code == 200:
                    return response
                    
            except requests.exceptions.RequestException as e:
                print(f"âŒ {url[:35]}... (Deneme {attempt + 1}/{retries}): {str(e)[:50]}...")
                if attempt < retries - 1:
                    time.sleep(1)  # KÄ±sa bekleme
                    
        return None

    def clean_text(self, text):
        """Metni temizle ve normalize et"""
        if not text:
            return ""
        
        # HTML entities decode
        text = text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
        text = text.replace('&quot;', '"').replace('&#39;', "'")
        text = text.replace('&nbsp;', ' ').replace('&ndash;', '-').replace('&mdash;', 'â€”')
        
        # Fazla boÅŸluklarÄ± temizle
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', '\n', text)
        
        return text.strip()

    def clean_title(self, title):
        """BaÅŸlÄ±ÄŸÄ± temizle"""
        if not title:
            return ""
        
        # Gereksiz kelimeler
        unwanted = ['Son Dakika:', 'CANLI:', 'VIDEO:', 'FOTO:', 'GALERÄ°:', 'Ã–ZEL:', 'HABER:', 'SON DAKÄ°KA:']
        
        for word in unwanted:
            title = title.replace(word, '').strip()
        
        return self.clean_text(title)

    def format_date(self, date_str):
        """Tarih string'ini standart formata Ã§evir"""
        if not date_str:
            return datetime.now().strftime('%d.%m.%Y %H:%M')
        
        try:
            # TÃ¼rkÃ§e aylarÄ± deÄŸiÅŸtir
            turkish_months = {
                'Ocak': '01', 'Åubat': '02', 'Mart': '03', 'Nisan': '04',
                'MayÄ±s': '05', 'Haziran': '06', 'Temmuz': '07', 'AÄŸustos': '08',
                'EylÃ¼l': '09', 'Ekim': '10', 'KasÄ±m': '11', 'AralÄ±k': '12'
            }
            
            for tr_month, num_month in turkish_months.items():
                date_str = date_str.replace(tr_month, num_month)
            
            # FarklÄ± tarih formatlarÄ±nÄ± dene
            formats = [
                '%d.%m.%Y %H:%M',
                '%d/%m/%Y %H:%M',
                '%Y-%m-%d %H:%M:%S',
                '%d %m %Y %H:%M',
                '%d.%m.%Y',
                '%d/%m/%Y',
                '%Y-%m-%d'
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str.strip(), fmt)
                    return dt.strftime('%d.%m.%Y %H:%M')
                except:
                    continue
                    
            return self.clean_text(date_str)
            
        except:
            return datetime.now().strftime('%d.%m.%Y %H:%M')

    def is_istanbul_related(self, title, description="", content=""):
        """Haberin Ä°stanbul ile ilgili olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        text_to_check = f"{title} {description} {content}".lower()
        
        return any(keyword.lower() in text_to_check for keyword in self.istanbul_keywords)

    def is_today_news(self, date_str, hours_back=24):
        """Haberin bugÃ¼n veya son X saat iÃ§inde olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        try:
            if not date_str:
                return True  # Tarih yoksa kabul et
                
            # FarklÄ± formatlarÄ± dene
            for fmt in ['%d.%m.%Y %H:%M', '%d.%m.%Y', '%Y-%m-%d %H:%M:%S']:
                try:
                    news_date = datetime.strptime(date_str, fmt)
                    time_diff = datetime.now() - news_date
                    return time_diff <= timedelta(hours=hours_back)
                except:
                    continue
            
            return True  # Parse edilemezse kabul et
            
        except:
            return True

    def generate_news_id(self, title, url):
        """Haber iÃ§in unique ID oluÅŸtur"""
        content = f"{title}{url}".encode('utf-8')
        return hashlib.md5(content).hexdigest()

    def scrape_sondakika(self, urls):
        """Sondakika.com'dan haber Ã§ek"""
        print("ğŸ“° Sondakika.com Ã§ekiliyor...")
        haberler = []
        
        for url in urls:
            try:
                response = self.safe_get(url)
                if not response:
                    continue
                    
                soup = BeautifulSoup(response.text, 'html.parser')
                haber_listesi = soup.select('li.nws')
                
                for haber in haber_listesi[:30]:  # Ä°lk 30 haber
                    try:
                        # BaÅŸlÄ±k
                        baslik_elem = haber.select_one('span.title')
                        if not baslik_elem:
                            continue
                        baslik = self.clean_title(baslik_elem.get_text())
                        
                        # Link
                        link_elem = haber.select_one('a.content')
                        if not link_elem or not link_elem.get('href'):
                            continue
                        link = "https://www.sondakika.com" + link_elem['href']
                        
                        # Ä°stanbul kontrolÃ¼
                        aciklama_elem = haber.select_one('p.news-detail')
                        aciklama = self.clean_text(aciklama_elem.get_text()) if aciklama_elem else ""
                        
                        if not self.is_istanbul_related(baslik, aciklama):
                            continue
                        
                        # Tarih
                        tarih_elem = haber.select_one('span.mdate')
                        tarih = self.format_date(tarih_elem.get_text()) if tarih_elem else ""
                        
                        # GÃ¼ncel haber kontrolÃ¼
                        if not self.is_today_news(tarih):
                            continue
                        
                        # Resim
                        resim_elem = haber.select_one('img')
                        resim = ""
                        if resim_elem:
                            resim = resim_elem.get('src') or resim_elem.get('data-originalm') or ""
                            if resim and resim.startswith('/'):
                                resim = "https://www.sondakika.com" + resim
                        
                        haber_data = {
                            'id': self.generate_news_id(baslik, link),
                            'baslik': baslik,
                            'link': link,
                            'aciklama': aciklama,
                            'tarih': tarih,
                            'resim': resim,
                            'kaynak': 'sondakika',
                            'site_url': 'www.sondakika.com',
                            'durum': 'yeni'
                        }
                        
                        haberler.append(haber_data)
                        
                    except Exception as e:
                        continue
                        
            except Exception as e:
                print(f"âŒ Sondakika hata: {str(e)[:50]}...")
        
        print(f"âœ… Sondakika: {len(haberler)} haber")
        return haberler

    def scrape_generic_site(self, site_name, urls):
        """Genel site scraper"""
        print(f"ğŸ“° {site_name.title()} Ã§ekiliyor...")
        haberler = []
        
        for url in urls:
            try:
                response = self.safe_get(url)
                if not response:
                    continue
                    
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Genel selector'lar
                selectors = [
                    'article', 'div.news-item', 'div.card', 'li.news',
                    '.news-card', '.story', '.post', '.news-list-item',
                    'div[class*="news"]', 'div[class*="haber"]', 'a[href*="/haber"]'
                ]
                
                haber_listesi = []
                for selector in selectors:
                    haber_listesi = soup.select(selector)
                    if len(haber_listesi) > 5:  # En az 5 element olsun
                        break
                
                base_url = self.news_sources[site_name]['base_url']
                
                for haber in haber_listesi[:30]:  # Ä°lk 30 element
                    try:
                        if haber.name == 'a':
                            link_elem = haber
                            baslik = self.clean_title(haber.get_text())
                        else:
                            link_elem = haber.select_one('a')
                            if not link_elem:
                                continue
                            baslik_elem = haber.select_one('h1, h2, h3, h4, .title, .headline, .baslik')
                            baslik = self.clean_title(baslik_elem.get_text()) if baslik_elem else self.clean_title(link_elem.get_text())
                        
                        if not baslik or len(baslik) < 10:
                            continue
                        
                        link = link_elem.get('href', '')
                        if not link:
                            continue
                            
                        if link.startswith('/'):
                            link = base_url + link
                        elif not link.startswith('http'):
                            continue
                        
                        # Ä°stanbul kontrolÃ¼
                        aciklama_elem = haber.select_one('.summary, .excerpt, .description, p, .spot')
                        aciklama = self.clean_text(aciklama_elem.get_text()) if aciklama_elem else ""
                        
                        if not self.is_istanbul_related(baslik, aciklama):
                            continue
                        
                        # Tarih
                        tarih_elem = haber.select_one('.date, .time, time, .tarih, .zaman')
                        tarih = self.format_date(tarih_elem.get_text()) if tarih_elem else ""
                        
                        if not self.is_today_news(tarih):
                            continue
                        
                        # Resim
                        resim_elem = haber.select_one('img')
                        resim = ""
                        if resim_elem:
                            resim = resim_elem.get('src') or resim_elem.get('data-src') or resim_elem.get('data-lazy-src') or ""
                            if resim and resim.startswith('/'):
                                resim = base_url + resim
                        
                        haber_data = {
                            'id': self.generate_news_id(baslik, link),
                            'baslik': baslik,
                            'link': link,
                            'aciklama': aciklama,
                            'tarih': tarih,
                            'resim': resim,
                            'kaynak': site_name,
                            'site_url': urlparse(base_url).netloc,
                            'durum': 'yeni'
                        }
                        
                        haberler.append(haber_data)
                        
                    except Exception as e:
                        continue
                        
            except Exception as e:
                print(f"âŒ {site_name.title()} hata: {str(e)[:50]}...")
        
        print(f"âœ… {site_name.title()}: {len(haberler)} haber")
        return haberler

    def scrape_sozcu(self, urls):
        """SÃ¶zcÃ¼'den haber Ã§ek"""
        return self.scrape_generic_site('sozcu', urls)

    def scrape_hurriyet(self, urls):
        """HÃ¼rriyet'ten haber Ã§ek"""
        return self.scrape_generic_site('hurriyet', urls)

    def scrape_single_source(self, source_name, source_config):
        """Tek bir haber kaynaÄŸÄ±ndan haber Ã§ek"""
        if not source_config.get('enabled', False):
            return []
        
        try:
            urls = []
            for key, value in source_config.items():
                if key.endswith('_url') and value:
                    urls.append(value)
            
            if not urls:
                return []
            
            # HÄ±zlÄ± rate limiting
            time.sleep(random.uniform(0.1, 0.5))
            
            # Ã–zel scraper'lar
            if source_name == 'sondakika':
                return self.scrape_sondakika(urls)
            elif source_name == 'sozcu':
                return self.scrape_sozcu(urls)
            elif source_name == 'hurriyet':
                return self.scrape_hurriyet(urls)
            else:
                return self.scrape_generic_site(source_name, urls)
                
        except Exception as e:
            print(f"âŒ {source_name} genel hatasÄ±: {str(e)[:50]}...")
            return []

    def remove_duplicates(self, haberler):
        """Dublicate haberleri kaldÄ±r"""
        seen_ids = set()
        seen_titles = set()
        unique_haberler = []
        
        for haber in haberler:
            haber_id = haber.get('id', '')
            baslik = haber.get('baslik', '').lower().strip()
            
            # ID veya benzer baÅŸlÄ±k kontrolÃ¼
            title_similarity = any(
                self.title_similarity(baslik, seen_title) > 0.8 
                for seen_title in seen_titles
            )
            
            if haber_id not in seen_ids and not title_similarity:
                seen_ids.add(haber_id)
                seen_titles.add(baslik)
                unique_haberler.append(haber)
        
        return unique_haberler

    def title_similarity(self, title1, title2):
        """Ä°ki baÅŸlÄ±k arasÄ±ndaki benzerliÄŸi hesapla (basit)"""
        if not title1 or not title2:
            return 0
        
        words1 = set(title1.lower().split())
        words2 = set(title2.lower().split())
        
        if not words1 or not words2:
            return 0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0

    def sort_news_by_priority(self, haberler):
        """Haberleri Ã¶nceliklerine gÃ¶re sÄ±rala"""
        def priority_score(haber):
            score = 0
            
            # Kaynak Ã¶nceliÄŸi
            source_priority = {
                'sondakika': 10, 'sozcu': 9, 'hurriyet': 8, 'milliyet': 7,
                'cnnturk': 6, 'ntv': 5, 'haberturk': 4, 'cumhuriyet': 3
            }
            score += source_priority.get(haber.get('kaynak', ''), 1)
            
            # Tarih Ã¶nceliÄŸi (yeni haberler)
            try:
                tarih_str = haber.get('tarih', '')
                if tarih_str:
                    tarih = datetime.strptime(tarih_str, '%d.%m.%Y %H:%M')
                    hours_ago = (datetime.now() - tarih).total_seconds() / 3600
                    score += max(0, 10 - hours_ago)  # Son 10 saat iÃ§indeki haberler
            except:
                pass
            
            # Ä°Ã§erik kalitesi
            baslik_len = len(haber.get('baslik', ''))
            aciklama_len = len(haber.get('aciklama', ''))
            
            if baslik_len > 20:
                score += 2
            if aciklama_len > 50:
                score += 3
            if haber.get('resim'):
                score += 1
            
            return score
        
        return sorted(haberler, key=priority_score, reverse=True)

    def scrape_all_sources(self, max_workers=3):
        """TÃ¼m haber kaynaklarÄ±ndan haberleri Ã§ek - Konservatif ayarlar"""
        print("ğŸŒ TÃ¼m haber kaynaklarÄ± Ã§ekiliyor...")
        start_time = time.time()
        
        all_haberler = []
        
        # Ã–nce aktif siteleri kontrol et
        active_sources = [name for name, config in self.news_sources.items() 
                         if config.get('enabled', False)]
        print(f"ğŸ“Š Aktif siteler: {', '.join(active_sources)}")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_source = {
                executor.submit(self.scrape_single_source, name, config): name
                for name, config in self.news_sources.items()
                if config.get('enabled', False)
            }
            
            for future in as_completed(future_to_source):
                source_name = future_to_source[future]
                try:
                    haberler = future.result(timeout=45)  # Thread timeout
                    if haberler:
                        all_haberler.extend(haberler)
                        print(f"âœ… {source_name}: {len(haberler)} haber eklendi")
                    else:
                        print(f"âš ï¸ {source_name}: Haber bulunamadÄ±")
                except Exception as e:
                    print(f"âŒ {source_name} thread hatasÄ±: {str(e)[:50]}...")
        
        # Dublicate'leri kaldÄ±r
        original_count = len(all_haberler)
        all_haberler = self.remove_duplicates(all_haberler)
        removed_count = original_count - len(all_haberler)
        
        # Ã–ncelikle sÄ±rala
        all_haberler = self.sort_news_by_priority(all_haberler)
        
        elapsed_time = time.time() - start_time
        print(f"\nğŸ“Š SONUÃ‡ Ã–ZET:")
        print(f"â±ï¸  SÃ¼re: {elapsed_time:.1f} saniye")
        print(f"ğŸ”„ {removed_count} dublicate kaldÄ±rÄ±ldÄ±")
        print(f"ğŸ¯ Toplam: {len(all_haberler)} benzersiz Ä°stanbul haberi")
        
        # Kaynak daÄŸÄ±lÄ±mÄ±
        source_count = {}
        for haber in all_haberler:
            kaynak = haber.get('kaynak', 'bilinmiyor')
            source_count[kaynak] = source_count.get(kaynak, 0) + 1
        
        if source_count:
            print(f"ğŸ“° Kaynak DaÄŸÄ±lÄ±mÄ±:")
            for kaynak, count in sorted(source_count.items(), key=lambda x: x[1], reverse=True):
                print(f"   â€¢ {kaynak}: {count} haber")
        
        return all_haberler

    def get_fresh_istanbul_news(self, hours_back=6, max_news=100):
        """Son X saat iÃ§indeki taze Ä°stanbul haberlerini getir"""
        print(f"ğŸ”¥ Son {hours_back} saat iÃ§indeki taze Ä°stanbul haberleri alÄ±nÄ±yor...")
        
        all_news = self.scrape_all_sources()
        
        # Tarih filtresi uygula
        fresh_news = []
        for news in all_news:
            if self.is_today_news(news.get('tarih', ''), hours_back):
                fresh_news.append(news)
        
        # Limitle
        fresh_news = fresh_news[:max_news]
        
        print(f"âœ… {len(fresh_news)} taze haber bulundu")
        return fresh_news

# ESKÄ° SÄ°STEMLE UYUMLULUK FONKSÄ°YONLARI
def sirali_haber_kontrol(config, onceki_istanbul_haberler=None, onceki_guncel_haberler=None):
    """Eski sistemle uyumlu sÄ±ralÄ± haber kontrolÃ¼"""
    print("ğŸ” SÄ±ralÄ± haber kontrolÃ¼ baÅŸlÄ±yor (Yeni Ã‡oklu Site Sistemi)...")
    
    scraper = MultiNewsSource()
    
    # Taze Ä°stanbul haberlerini Ã§ek
    yeni_haberler = scraper.get_fresh_istanbul_news(
        hours_back=config.get('settings', {}).get('hours_back', 12),
        max_news=config.get('settings', {}).get('max_news', 100)
    )
    
    # Ã–nceki haberlerin linklerini Ã§Ä±kar
    onceki_linkler = set()
    if onceki_istanbul_haberler:
        onceki_linkler.update(h.get('link', '') for h in onceki_istanbul_haberler)
    if onceki_guncel_haberler:
        onceki_linkler.update(h.get('link', '') for h in onceki_guncel_haberler)
    
    # Yeni haberleri tespit et
    gercekten_yeni = []
    for haber in yeni_haberler:
        if haber.get('link') and haber['link'] not in onceki_linkler:
            haber['durum'] = 'yeni'
            gercekten_yeni.append(haber)
        else:
            haber['durum'] = 'eski'
    
    # SonuÃ§larÄ± logla
    if gercekten_yeni:
        print(f"ğŸ”¥ {len(gercekten_yeni)} yeni haber bulundu!")
        # TÃ¼m Ä°stanbul haberlerini gÃ¼ncelle
        for haber in yeni_haberler:
            if haber not in gercekten_yeni:
                haber['durum'] = 'eski'
        return yeni_haberler, [], gercekten_yeni, 'multi'
    else:
        print("ğŸ“° Yeni haber bulunamadÄ±")
        return yeni_haberler, [], [], 'none'

def tum_haberler_cek(config):
    """Eski sistemle uyumlu tÃ¼m haber Ã§ekme"""
    print("ğŸŒ TÃ¼m haber kaynaklarÄ± Ã§ekiliyor (Yeni Ã‡oklu Site Sistemi)...")
    
    scraper = MultiNewsSource()
    
    # AyarlarÄ± config'den al
    hours_back = config.get('settings', {}).get('hours_back', 12)
    max_news = config.get('settings', {}).get('max_news', 100)
    
    # TÃ¼m haberleri Ã§ek
    haberler = scraper.get_fresh_istanbul_news(hours_back=hours_back, max_news=max_news)
    
    print(f"ğŸ¯ Toplam {len(haberler)} Ä°stanbul haberi Ã§ekildi")
    return haberler

def haber_istatistikleri(haberler):
    """Haber listesi istatistiklerini dÃ¶ndÃ¼r"""
    if not haberler:
        return {"toplam": 0, "yeni": 0, "eski": 0, "kaynak_dagilimi": {}}
    
    toplam = len(haberler)
    yeni = len([h for h in haberler if h.get('durum') == 'yeni'])
    eski = toplam - yeni
    
    # Kaynak daÄŸÄ±lÄ±mÄ±
    kaynak_dagilimi = {}
    for haber in haberler:
        kaynak = haber.get('kaynak', 'bilinmiyor')
        kaynak_dagilimi[kaynak] = kaynak_dagilimi.get(kaynak, 0) + 1
    
    return {
        "toplam": toplam,
        "yeni": yeni, 
        "eski": eski,
        "kaynak_dagilimi": kaynak_dagilimi
    }

def safe_get(url, timeout=30, retries=3):
    """Eski sistemle uyumluluk iÃ§in safe_get fonksiyonu"""
    scraper = MultiNewsSource()
    return scraper.safe_get(url, timeout, retries)

def metin_temizle(metin):
    """Eski sistemle uyumluluk iÃ§in metin temizleme"""
    scraper = MultiNewsSource()
    return scraper.clean_text(metin)

def baslik_temizle(baslik):
    """Eski sistemle uyumluluk iÃ§in baÅŸlÄ±k temizleme"""
    scraper = MultiNewsSource()
    return scraper.clean_title(baslik)

def tarih_formatla(tarih_str):
    """Eski sistemle uyumluluk iÃ§in tarih formatlama"""
    scraper = MultiNewsSource()
    return scraper.format_date(tarih_str)

# Test ve kullanÄ±m
def test_multi_scraper():
    """Test fonksiyonu"""
    print("ğŸ§ª Ã‡oklu haber kaynaÄŸÄ± testi baÅŸlÄ±yor...")
    
    scraper = MultiNewsSource()
    
    # Taze Ä°stanbul haberlerini Ã§ek
    taze_haberler = scraper.get_fresh_istanbul_news(hours_back=12, max_news=50)
    
    if taze_haberler:
        print(f"\nğŸ“° Ä°lk 5 haber Ã¶rneÄŸi:")
        for i, haber in enumerate(taze_haberler[:5], 1):
            print(f"\n{i}. {haber.get('baslik', 'N/A')[:80]}...")
            print(f"   Kaynak: {haber.get('kaynak', 'N/A')} | Tarih: {haber.get('tarih', 'N/A')}")
            print(f"   Link: {haber.get('link', 'N/A')[:80]}...")
    
    return taze_haberler

def hizli_test():
    """HÄ±zlÄ± test - sadece Ã§alÄ±ÅŸan siteleri dene"""
    print("âš¡ HIZLI TEST BAÅLIYOR...")
    print("="*40)
    
    scraper = MultiNewsSource()
    
    # Sadece gÃ¼venilir siteleri etkinleÅŸtir
    scraper.news_sources['ntv']['enabled'] = False
    scraper.news_sources['cumhuriyet']['enabled'] = False
    
    print("ğŸ¯ Test edilecek siteler: Sondakika, SÃ¶zcÃ¼, HÃ¼rriyet, Milliyet, CNN TÃ¼rk, HabertÃ¼rk")
    
    try:
        # HÄ±zlÄ± test - 3 saat iÃ§indeki haberler
        haberler = scraper.get_fresh_istanbul_news(hours_back=3, max_news=30)
        
        if haberler:
            print(f"\nâœ… {len(haberler)} Ä°stanbul haberi bulundu!")
            print(f"\nğŸ“° Ä°lk 3 haber:")
            for i, haber in enumerate(haberler[:3], 1):
                print(f"\n{i}. {haber.get('baslik', 'N/A')[:60]}...")
                print(f"   Kaynak: {haber.get('kaynak', 'N/A')} | {haber.get('tarih', 'N/A')}")
        else:
            print("âŒ HiÃ§ haber bulunamadÄ±!")
            
    except Exception as e:
        print(f"âŒ Test hatasÄ±: {e}")
    
    print("\nâš¡ HÄ±zlÄ± test tamamlandÄ±!")

def test_eski_sistem_uyumlulugi():
    """Eski sistem uyumluluÄŸunu test et"""
    print("ğŸ”„ ESKÄ° SÄ°STEM UYUMLULUÄU TEST EDÄ°LÄ°YOR...")
    
    # Ã–rnek config
    config = {
        'settings': {
            'sondakika_url': 'https://www.sondakika.com/istanbul/',
            'hours_back': 6,
            'max_news': 30  # Test iÃ§in az haber
        }
    }
    
    try:
        print("\n1ï¸âƒ£ tum_haberler_cek test ediliyor...")
        haberler = tum_haberler_cek(config)
        print(f"âœ… {len(haberler)} haber Ã§ekildi")
        
        print("\n2ï¸âƒ£ haber_istatistikleri test ediliyor...")
        stats = haber_istatistikleri(haberler)
        print(f"âœ… Ä°statistikler: Toplam {stats['toplam']}, Yeni {stats['yeni']}")
        
        print("\n3ï¸âƒ£ sirali_haber_kontrol test ediliyor...")
        istanbul_h, guncel_h, yeni_h, durum = sirali_haber_kontrol(config, [], [])
        print(f"âœ… Kontrol sonucu: {len(yeni_h)} yeni haber, durum: {durum}")
        
        print("\nâœ… Eski sistem uyumluluÄŸu BAÅARILI!")
        return haberler
        
    except Exception as e:
        print(f"âŒ Eski sistem test hatasÄ±: {e}")
        return []

# Ana config iÃ§in gÃ¼ncelleme fonksiyonu
def get_updated_config():
    """GÃ¼ncellenmiÅŸ config dÃ¶ndÃ¼r"""
    return {
        'settings': {
            'hours_back': 12,  # Son 12 saat
            'max_news': 100,   # Maksimum haber sayÄ±sÄ±
            'max_workers': 3,  # Thread sayÄ±sÄ±
            'update_interval': 300,  # 5 dakika
            'enabled_sources': [
                'sondakika', 'sozcu', 'hurriyet', 'milliyet', 
                'cnnturk', 'haberturk', 'cumhuriyet'
            ]
        },
        'keywords': {
            'istanbul_filter': True,
            'today_filter': True,
            'duplicate_removal': True
        }
    }

if __name__ == "__main__":
    # HÄ±zlÄ± test modu
    print("ğŸš€ Ã‡oklu Site Ä°stanbul Haber Ã‡ekici v2.0")
    print("="*50)
    
    try:
        # 1. HÄ±zlÄ± test (Ã¶nerilen)
        print("\nğŸ”¥ HIZLI TEST (Ã¶nerilen):")
        hizli_test()
        
        # 2. Eski sistem uyumluluk testi
        print("\n" + "="*50)
        print("ğŸ”„ ESKÄ° SÄ°STEM UYUMLULUK TESTÄ°:")
        test_eski_sistem_uyumlulugi()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ KullanÄ±cÄ± tarafÄ±ndan durduruldu")
    except Exception as e:
        print(f"\nâŒ Beklenmeyen hata: {e}")
        
    print("\nâœ… TÃ¼m testler tamamlandÄ±!")
    print("\nğŸ’¡ KullanÄ±m Ã¶nerisi:")
    print("   â€¢ Normal kullanÄ±m iÃ§in: scraper.get_fresh_istanbul_news()")
    print("   â€¢ Eski kod uyumluluÄŸu iÃ§in: tum_haberler_cek(config)")
